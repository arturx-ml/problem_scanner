{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polymarket User Problems Analysis\n",
    "\n",
    "This notebook analyzes Reddit and Twitter posts to identify the top problems users face with Polymarket.\n",
    "\n",
    "## Objectives:\n",
    "1. Collect posts about Polymarket from Reddit (last 3 months) and Twitter (last 7 days)\n",
    "2. Filter for problem-related discussions\n",
    "3. Categorize and analyze problems using NLP\n",
    "4. Identify top 3-5 problems to build solutions for\n",
    "\n",
    "**Data Collection:**\n",
    "- **Reddit:** 300 posts per subreddit, last 90 days (3 months)\n",
    "- **Twitter:** 10 tweets minimum (API limit), last 7 days\n",
    "- **Total Expected:** ~100-200+ posts for comprehensive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from collect_polymarket_data import PolymarketDataCollector\n",
    "from analyze_polymarket_problems import PolymarketProblemAnalyzer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Collect Data from Reddit and Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "POLYMARKET DATA COLLECTION STARTING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Collecting Reddit posts about 'polymarket'\n",
      "============================================================\n",
      "\n",
      "[DEBUG] Date filter: Posts after 2025-10-11 22:32:37 UTC\n",
      "[DEBUG] Looking back 30 days\n",
      "\n",
      "Searching r/polymarket...\n",
      "  [DEBUG] Getting all posts (no search query needed), Limit: 100\n",
      "  [DEBUG] Post #1: 'Using API for sports...' | Date: 2025-11-04 | Score: 1\n",
      "  [DEBUG] Post #2: 'The Changing Landscape of Sports Betting...' | Date: 2025-10-30 | Score: 1\n",
      "  [DEBUG] Post #3: 'MoonPay for deposits?...' | Date: 2025-10-24 | Score: 1\n",
      "  [DEBUG] Filtered out (too old): 'Lord Miles dead yes or no?...' | Date: 2025-09-26\n",
      "  [DEBUG] Filtered out (too old): 'Parliament, Polymarket and the perils of political betting...' | Date: 2025-09-19\n",
      "  [DEBUG] Retrieved 51 posts from API\n",
      "  [DEBUG] Filtered out 41 posts (too old)\n",
      "  ‚úì Found 10 posts matching criteria\n",
      "\n",
      "Searching r/cryptocurrency...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  [DEBUG] Post #1: 'I built a website to track whales and insider/suspicious tra...' | Date: 2025-10-26 | Score: 348\n",
      "  [DEBUG] Post #2: 'Romania blacklists Polymarket for illegal crypto betting ami...' | Date: 2025-11-02 | Score: 95\n",
      "  [DEBUG] Post #3: 'Romanian Regulator Blacklists Polymarket as 'Gambling That M...' | Date: 2025-11-01 | Score: 41\n",
      "  [DEBUG] Retrieved 19 posts from API\n",
      "  [DEBUG] Filtered out 0 posts (too old)\n",
      "  ‚úì Found 19 posts matching criteria\n",
      "\n",
      "Searching r/CryptoMarkets...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  [DEBUG] Post #1: 'Is there a solution to polymarkets 'no contest' situation?...' | Date: 2025-10-30 | Score: 1\n",
      "  [DEBUG] Post #2: 'Polymarket to go live in the US by late November as sports b...' | Date: 2025-10-29 | Score: 3\n",
      "  [DEBUG] Post #3: 'Trump just announced 2000 dollar tariff dividend checks... h...' | Date: 2025-11-09 | Score: 135\n",
      "  [DEBUG] Retrieved 4 posts from API\n",
      "  [DEBUG] Filtered out 0 posts (too old)\n",
      "  ‚úì Found 4 posts matching criteria\n",
      "\n",
      "Searching r/defi...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  [DEBUG] Post #1: 'Maximize yield on my $100k ETH bag when I'm fine with the ET...' | Date: 2025-10-29 | Score: 25\n",
      "  [DEBUG] Post #2: '[Hackathon] Build Forecasting dApps on BNB Chain ‚Äî $400K+ in...' | Date: 2025-10-22 | Score: 1\n",
      "  [DEBUG] Retrieved 2 posts from API\n",
      "  [DEBUG] Filtered out 0 posts (too old)\n",
      "  ‚úì Found 2 posts matching criteria\n",
      "\n",
      "Searching r/ethereum...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  [DEBUG] Retrieved 0 posts from API\n",
      "  [DEBUG] Filtered out 0 posts (too old)\n",
      "  ‚úì Found 0 posts matching criteria\n",
      "\n",
      "Searching r/polygon...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  [DEBUG] Retrieved 0 posts from API\n",
      "  [DEBUG] Filtered out 0 posts (too old)\n",
      "  ‚úì Found 0 posts matching criteria\n",
      "\n",
      "Searching r/betting...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  ‚úó Error searching r/betting: received 403 HTTP response\n",
      "\n",
      "Searching r/prediction_markets...\n",
      "  [DEBUG] Query: 'polymarket', Limit: 100, Time filter: month\n",
      "  ‚úó Error searching r/prediction_markets: received 404 HTTP response\n",
      "\n",
      "============================================================\n",
      "Total Reddit posts collected: 35\n",
      "============================================================\n",
      "\n",
      "üí° Collecting only 3 tweets to minimize API usage (extremely conservative)\n",
      "\n",
      "============================================================\n",
      "Collecting Twitter posts about 'polymarket'\n",
      "============================================================\n",
      "\n",
      "[DEBUG] Date range: 2025-11-03 22:32:54 to 2025-11-10 22:31:54 UTC\n",
      "[DEBUG] Searching last 7 days\n",
      "\n",
      "  [DEBUG] Batch #1: Requesting 3 tweets (max_results=3)...\n",
      "  [DEBUG] API start_time: 2025-11-03T22:32:54.000Z\n",
      "  [DEBUG] API end_time: 2025-11-10T22:31:54.000Z\n",
      "\n",
      "  ‚úó Error collecting tweets: 400 Bad Request\n",
      "The `max_results` query parameter value [3] is not between 10 and 100\n",
      "  ‚ÑπÔ∏è  Error type: BadRequest\n",
      "\n",
      "============================================================\n",
      "Total Twitter posts collected: 0\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DATA COLLECTION COMPLETE\n",
      "============================================================\n",
      "Total posts collected: 35\n",
      "  - Reddit: 35\n",
      "  - Twitter: 0\n",
      "Data saved to: polymarket_data_20251111_003254.csv\n",
      "============================================================\n",
      "\n",
      "\n",
      "Collected 35 total posts\n",
      "\n",
      "Platform breakdown:\n",
      "platform\n",
      "reddit    35\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize collector\n",
    "collector = PolymarketDataCollector()\n",
    "\n",
    "# Collect data from Reddit (3 months) and Twitter (7 days)\n",
    "print(\"üöÄ Collecting comprehensive data...\\n\")\n",
    "print(\"üìä Reddit: 300 posts/subreddit, last 90 days\")\n",
    "print(\"üê¶ Twitter: 10 tweets minimum (skip if quota exceeded)\\n\")\n",
    "\n",
    "df = collector.collect_all_data(days_back=90, skip_twitter=True)  # Skip Twitter due to quota\n",
    "\n",
    "print(f\"\\n‚úÖ Collected {len(df)} total posts\")\n",
    "print(f\"\\nüìà Platform breakdown:\")\n",
    "print(df['platform'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Data Overview:\")\n",
    "print(f\"Date range: {df['created_utc'].min()} to {df['created_utc'].max()}\")\n",
    "print(f\"\\nPlatforms: {df['platform'].unique()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify Problem Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = PolymarketProblemAnalyzer()\n",
    "\n",
    "# Use the collected data\n",
    "analyzer.df = df\n",
    "\n",
    "# Identify problem posts\n",
    "problem_df = analyzer.identify_problem_posts()\n",
    "\n",
    "print(f\"\\nFound {len(problem_df)} problem-related posts out of {len(df)} total\")\n",
    "print(f\"That's {len(problem_df)/len(df)*100:.1f}% of all posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample problem posts\n",
    "print(\"Sample problem posts:\\n\")\n",
    "for i, row in problem_df.head(5).iterrows():\n",
    "    print(f\"Platform: {row['platform']}\")\n",
    "    print(f\"Date: {row['created_utc']}\")\n",
    "    print(f\"Text: {row['full_text'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment\n",
    "problem_df = analyzer.analyze_sentiment(problem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment counts\n",
    "sentiment_counts = problem_df['sentiment'].value_counts()\n",
    "axes[0].bar(sentiment_counts.index, sentiment_counts.values, \n",
    "           color=['red', 'gray', 'green'])\n",
    "axes[0].set_title('Sentiment Distribution of Problem Posts')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Sentiment scores\n",
    "axes[1].hist(problem_df['sentiment_compound'], bins=30, edgecolor='black')\n",
    "axes[1].set_title('Sentiment Score Distribution')\n",
    "axes[1].set_xlabel('Compound Score (-1 to 1)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', label='Neutral')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Categorize Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize problems\n",
    "problem_df, category_counts = analyzer.categorize_problems(problem_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize problem categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_10_categories = dict(category_counts.most_common(10))\n",
    "plt.barh(list(top_10_categories.keys()), list(top_10_categories.values()),\n",
    "         color=sns.color_palette(\"Reds_r\", 10))\n",
    "plt.xlabel('Number of Mentions')\n",
    "plt.title('Top 10 Problem Categories')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract Top 5 Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 5 problems\n",
    "top_problems = analyzer.extract_top_problems(problem_df, category_counts, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed information for each top problem\n",
    "for problem in top_problems:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"RANK #{problem['rank']}: {problem['category'].upper().replace('_', ' ')}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Mentions: {problem['count']} ({problem['percentage']:.1f}% of problem posts)\")\n",
    "    print(f\"Average Sentiment: {problem['avg_sentiment']:.3f}\")\n",
    "    print(f\"Platforms: {problem['platforms']}\")\n",
    "    print(\"\\nSample Complaints:\")\n",
    "    for i, complaint in enumerate(problem['sample_complaints'][:2], 1):\n",
    "        print(f\"\\n{i}. {complaint[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "viz_file = analyzer.visualize_results(top_problems, category_counts)\n",
    "print(f\"Visualization saved to: {viz_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Full Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text report\n",
    "report_file = analyzer.generate_report(top_problems)\n",
    "print(f\"Report saved to: {report_file}\")\n",
    "\n",
    "# Display report\n",
    "with open(report_file, 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results for App Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "problem_file = f'polymarket_problem_posts_{timestamp}.csv'\n",
    "problem_df.to_csv(problem_file, index=False)\n",
    "print(f\"Problem posts saved to: {problem_file}\")\n",
    "\n",
    "# Save top problems as JSON for easy access\n",
    "import json\n",
    "json_file = f'polymarket_top_problems_{timestamp}.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    # Convert to serializable format\n",
    "    problems_json = [{\n",
    "        'rank': p['rank'],\n",
    "        'category': p['category'],\n",
    "        'count': p['count'],\n",
    "        'percentage': p['percentage'],\n",
    "        'avg_sentiment': p['avg_sentiment'],\n",
    "        'platforms': p['platforms']\n",
    "    } for p in top_problems]\n",
    "    json.dump(problems_json, f, indent=2)\n",
    "    \n",
    "print(f\"Top problems JSON saved to: {json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "The analysis has identified the top 5 problems users face with Polymarket.\n",
    "\n",
    "### Next Steps for App Development:\n",
    "1. Review the top problems identified\n",
    "2. Prioritize which problems to solve based on:\n",
    "   - Frequency (number of mentions)\n",
    "   - Sentiment severity (how negative)\n",
    "   - Feasibility of solution using Polymarket + Polygon API\n",
    "3. Design and build apps to address these problems\n",
    "4. Test solutions with the community\n",
    "\n",
    "### Files Generated:\n",
    "- Problem posts CSV (for further analysis)\n",
    "- Text report (detailed findings)\n",
    "- Visualization PNG (charts and graphs)\n",
    "- JSON file (easy access to top problems)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
